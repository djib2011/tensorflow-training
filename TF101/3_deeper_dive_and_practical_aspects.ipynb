{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43fe1a8",
   "metadata": {},
   "source": [
    "# 3. Deeper dive into model architectures and practical aspacts in training\n",
    "\n",
    "Now that we have a good understanding how the keras API works we'll mainly work on two things:\n",
    "\n",
    "1. Building more complex architectures.\n",
    "    - What happens if not all inputs are numerical?\n",
    "    - How can we use inputs of more than one data type?\n",
    "    - What are Embedding layers and how can they help us?\n",
    "2. Practical aspects regarding model training.\n",
    "    - What is the history callback and how can we use it?\n",
    "    - How can we visualize how our model is doing during training?\n",
    "3. Bonus practical aspects:  \n",
    "    - What is model calibration and how can we visualize it?\n",
    "    - How can we perform hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f5d27ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__[0] == '2', 'this tutorial is for tensorflow versions of 2 or higher'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5e756",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To better demonstrate the topics that we want to discuss, we'll use a different dataset than the toy example we've seen up till now. The dataset is called *airlines_delay* and can be found in [Kaggle](https://www.kaggle.com/datasets/jimschacko/airlines-dataset-to-predict-a-delay). This dataset consists of 7 features (4 numerical, 3 string) and the goal of this dataset is to predict if a flight will be delayed (essentially a binary classification task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5cd0a194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flight</th>\n",
       "      <th>Time</th>\n",
       "      <th>Length</th>\n",
       "      <th>Airline</th>\n",
       "      <th>AirportFrom</th>\n",
       "      <th>AirportTo</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2313.0</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>DL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>HOU</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6948.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>OO</td>\n",
       "      <td>COS</td>\n",
       "      <td>ORD</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1247.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>BOS</td>\n",
       "      <td>CLT</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>US</td>\n",
       "      <td>OGG</td>\n",
       "      <td>PHX</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>563.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>BMI</td>\n",
       "      <td>ATL</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539377</th>\n",
       "      <td>6973.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>OO</td>\n",
       "      <td>GEG</td>\n",
       "      <td>SEA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539378</th>\n",
       "      <td>1264.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>WN</td>\n",
       "      <td>LAS</td>\n",
       "      <td>DEN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539379</th>\n",
       "      <td>5209.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>EV</td>\n",
       "      <td>CAE</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539380</th>\n",
       "      <td>607.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>WN</td>\n",
       "      <td>BWI</td>\n",
       "      <td>BUF</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539381</th>\n",
       "      <td>6377.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>OO</td>\n",
       "      <td>CPR</td>\n",
       "      <td>DEN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>539382 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Flight    Time  Length Airline AirportFrom AirportTo  DayOfWeek  Class\n",
       "0       2313.0  1296.0   141.0      DL         ATL       HOU          1      0\n",
       "1       6948.0   360.0   146.0      OO         COS       ORD          4      0\n",
       "2       1247.0  1170.0   143.0      B6         BOS       CLT          3      0\n",
       "3         31.0  1410.0   344.0      US         OGG       PHX          6      0\n",
       "4        563.0   692.0    98.0      FL         BMI       ATL          4      0\n",
       "...        ...     ...     ...     ...         ...       ...        ...    ...\n",
       "539377  6973.0   530.0    72.0      OO         GEG       SEA          5      1\n",
       "539378  1264.0   560.0   115.0      WN         LAS       DEN          4      1\n",
       "539379  5209.0   827.0    74.0      EV         CAE       ATL          2      1\n",
       "539380   607.0   715.0    65.0      WN         BWI       BUF          4      1\n",
       "539381  6377.0   770.0    55.0      OO         CPR       DEN          2      1\n",
       "\n",
       "[539382 rows x 8 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/airlines_delay.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f316e",
   "metadata": {},
   "source": [
    "The thing that is going to give us the most trouble is the 3 categorical variables it has, `Airline`, `AirportFrom` and `AirportTo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98132ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN    94097\n",
      "DL    60940\n",
      "OO    50254\n",
      "AA    45656\n",
      "MQ    36604\n",
      "US    34500\n",
      "XE    31126\n",
      "EV    27983\n",
      "UA    27619\n",
      "CO    21118\n",
      "FL    20827\n",
      "9E    20686\n",
      "B6    18112\n",
      "YV    13725\n",
      "OH    12630\n",
      "AS    11471\n",
      "F9     6456\n",
      "HA     5578\n",
      "Name: Airline, dtype: int64 \n",
      "\n",
      "ATL    34449\n",
      "ORD    24822\n",
      "DFW    22153\n",
      "DEN    19843\n",
      "LAX    16657\n",
      "       ...  \n",
      "MMH       16\n",
      "SJT       15\n",
      "GUM       10\n",
      "ADK        9\n",
      "ABR        2\n",
      "Name: AirportFrom, Length: 293, dtype: int64 \n",
      "\n",
      "ATL    34440\n",
      "ORD    24871\n",
      "DFW    22153\n",
      "DEN    19848\n",
      "LAX    16656\n",
      "       ...  \n",
      "MMH       16\n",
      "SJT       15\n",
      "GUM       10\n",
      "ADK        9\n",
      "ABR        2\n",
      "Name: AirportTo, Length: 293, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Airline'].value_counts(), '\\n')\n",
    "print(data['AirportFrom'].value_counts(), '\\n')\n",
    "print(data['AirportTo'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ddd1a",
   "metadata": {},
   "source": [
    "## Part 1: Deeper dive into model architectures\n",
    "\n",
    "### Attempt 1: Ignore categorical features\n",
    "\n",
    "For our first attempt we'll completely ignore these categorical features and only deal with the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ce2611dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Airline', 'AirportFrom', 'AirportTo', 'Class'])\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "879dcb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2950/2950 [==============================] - 11s 3ms/step - loss: 2.1095 - accuracy: 0.5192 - precision: 0.4601 - recall: 0.4570 - auc: 0.5175 - val_loss: 0.9150 - val_accuracy: 0.5153 - val_precision: 0.4698 - val_recall: 0.6879 - val_auc: 0.5606\n",
      "Epoch 2/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.9316 - accuracy: 0.5332 - precision: 0.4740 - recall: 0.4364 - auc: 0.5300 - val_loss: 0.7457 - val_accuracy: 0.5599 - val_precision: 0.5729 - val_recall: 0.0461 - val_auc: 0.5586\n",
      "Epoch 3/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.7600 - accuracy: 0.5453 - precision: 0.4878 - recall: 0.4141 - auc: 0.5437 - val_loss: 0.7101 - val_accuracy: 0.5322 - val_precision: 0.4842 - val_recall: 0.7725 - val_auc: 0.5721\n",
      "Epoch 4/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.7045 - accuracy: 0.5590 - precision: 0.5067 - recall: 0.3804 - auc: 0.5607 - val_loss: 0.6865 - val_accuracy: 0.5588 - val_precision: 0.5039 - val_recall: 0.6074 - val_auc: 0.5830\n",
      "Epoch 5/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.6769 - accuracy: 0.5726 - precision: 0.5366 - recall: 0.2977 - auc: 0.5873 - val_loss: 0.6769 - val_accuracy: 0.5659 - val_precision: 0.5590 - val_recall: 0.1195 - val_auc: 0.5947\n",
      "Epoch 6/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.6760 - accuracy: 0.5726 - precision: 0.5359 - recall: 0.3030 - auc: 0.5898 - val_loss: 0.6763 - val_accuracy: 0.5710 - val_precision: 0.5687 - val_recall: 0.1515 - val_auc: 0.5958\n",
      "Epoch 7/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.6757 - accuracy: 0.5729 - precision: 0.5363 - recall: 0.3051 - auc: 0.5909 - val_loss: 0.6750 - val_accuracy: 0.5663 - val_precision: 0.5585 - val_recall: 0.1241 - val_auc: 0.6004\n",
      "Epoch 8/10\n",
      "2950/2950 [==============================] - 9s 3ms/step - loss: 0.6755 - accuracy: 0.5738 - precision: 0.5379 - recall: 0.3067 - auc: 0.5917 - val_loss: 0.6744 - val_accuracy: 0.5778 - val_precision: 0.5290 - val_recall: 0.4726 - val_auc: 0.6030\n",
      "Epoch 9/10\n",
      "2950/2950 [==============================] - 9s 3ms/step - loss: 0.6742 - accuracy: 0.5748 - precision: 0.5396 - recall: 0.3112 - auc: 0.5960 - val_loss: 0.6721 - val_accuracy: 0.5797 - val_precision: 0.5409 - val_recall: 0.3709 - val_auc: 0.6038\n",
      "Epoch 10/10\n",
      "2950/2950 [==============================] - 8s 3ms/step - loss: 0.6738 - accuracy: 0.5761 - precision: 0.5409 - recall: 0.3199 - auc: 0.5977 - val_loss: 0.6732 - val_accuracy: 0.5771 - val_precision: 0.5377 - val_recall: 0.3592 - val_auc: 0.5996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f934295bb50>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tf.keras.layers.Input((4,))\n",
    "hid1 = tf.keras.layers.Dense(300, activation='relu')(inp)\n",
    "hid2 = tf.keras.layers.Dense(100, activation='relu')(hid1)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(hid2)\n",
    "\n",
    "model = tf.keras.models.Model(inp, out)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',\n",
    "                                                                     'Precision',  # for some reason \n",
    "                                                                     'Recall',     # these are \n",
    "                                                                     'AUC'])       # case sensitive\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fccfd3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5057/5057 [==============================] - 6s 1ms/step - loss: 0.6732 - accuracy: 0.5771 - precision: 0.5377 - recall: 0.3592 - auc: 0.5996\n",
      "Loss      : 0.67\n",
      "Accuracy  : 0.58\n",
      "Precision : 0.54\n",
      "Recall    : 0.36\n",
      "AUC       : 0.60\n"
     ]
    }
   ],
   "source": [
    "metrics = ['Loss', 'Accuracy', 'Precision', 'Recall', 'AUC']\n",
    "\n",
    "results = model.evaluate(X_test, y_test)\n",
    "\n",
    "for name, value in zip(metrics, results):\n",
    "    print(f'{name:<10}: {value:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e171af4",
   "metadata": {},
   "source": [
    "Arguably we're not great really great results in this setup. We could try to tune the architecture or other hyperparams like the learning rate more, but I don't think this would lead to a significant boost in performance. A much more promissing direction would be to try to incorporate the other features to the model.\n",
    "\n",
    "### Attempt 2: Embed categorical features\n",
    "\n",
    "We want to utilize the remaining features of the model, however these are in a form not understandable by our network, i.e. **categorical**. The most common way to deal with this issue is to **represent each catebory by a fixed length vector**. These vectors are called **embeddings** and are **fully trainable**. But how does this work?\n",
    "\n",
    "Before we begin, we need to define a **vocabulary size** (let's call this $V$) and an **embedding dim** (let's say this is $D$. The second is simply the size of each embedding (i.e. how many dims will the vector that represents each category have). The first shows how many categories will get their own, dedicated embedding. In features that don't have too many unique values, this is set to be the same as the cardinality of the feature (i.e. every unique value gets its own dedicated embedding). If the feature has too many unique values, only the $V$ most frequent categories will get their own embedding. The remaining will usually all be represented by a single embedding that we call OOV (i.e. out-of-vocabulary). Keras calls these two properites `input_dim` and `output_dim` respectively.\n",
    "\n",
    "Internally, a lookup table of dimensions $V \\times D$ is created, where each row refers to a category. All of these parameters are trainable! When the network sees a specific input, it looks up the $D$-dimensional embedding of that input and feeds it to the next layer. \n",
    "\n",
    "![](https://github.com/djib2011/tensorflow-training/blob/main/figures/embedding.png)\n",
    "\n",
    "In keras this is implemented through the [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer. The embedding layer doesn't work by default on string inputs, though. They first need to be encoded as integers. For this purpose we will use the [`StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layer.\n",
    "\n",
    "How can we use embeddings in our case, though?\n",
    "\n",
    "There are a few things to notice in our case:\n",
    "- We have very small vocabulary sizes. This means that we can have dedicated embeddings for the whole vocabulary$^1$\n",
    "- The two airport features have the same exact vocabulary, so we will use a single embedding table for both of these.\n",
    "- We have both numerical and categorical features we wan't to use. This means that we'll need to embed the categories (each feature separately) and then concatenate the embeddings with the numeric features. Let's say we use an embedding size of $3$ for the airline and $5$ for each of the airport features. The concatenated vector that will be fed to the dense layers will have $17$ dims: $3$ (Airline) $+5$ (AirportFrom) $+5$ (AirportTo) $+4$ (numeric) $=17$.\n",
    "\n",
    "$^1$ *Note: this isn't a good practive, as some categories that don't have many samples will not get many updates for their embeddings, leaving them undertrained. Because this requires tuning, though, we won't play with OOV embeddings at all.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "aaa7b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for the 4 numeric features of the dataset\n",
    "numeric_inp = tf.keras.layers.Input((4,))  # shape --> (batch, 4)\n",
    "\n",
    "# Make separate inputs for the 3 categorical features\n",
    "airline_inp = tf.keras.layers.Input((1,), dtype=tf.string)\n",
    "airport_from_inp = tf.keras.layers.Input((1,), dtype=tf.string)\n",
    "airport_to_inp = tf.keras.layers.Input((1,), dtype=tf.string)\n",
    "\n",
    "# Create lookup tables mapping the strings to integers\n",
    "airline_look = tf.keras.layers.StringLookup(vocabulary=data['Airline'].unique())\n",
    "airport_look = tf.keras.layers.StringLookup(vocabulary=data['AirportTo'].unique())\n",
    "\n",
    "# Encode the 3 categorical features using the lookup tables\n",
    "airline_encoded = airline_look(airline_inp)\n",
    "airport_from_encoded = airport_look(airport_from_inp)\n",
    "airport_to_encoded = airport_look(airport_to_inp)\n",
    "\n",
    "# Create embedding tables for each of the 3 categorical features\n",
    "airline_emb = tf.keras.layers.Embedding(input_dim=len(data['Airline'].unique())+1,  \n",
    "                                        output_dim=3)  # the +1 is for the OOV embedding\n",
    "airport_emb = tf.keras.layers.Embedding(input_dim=len(data['AirportTo'].unique())+1,\n",
    "                                        output_dim=5)\n",
    "\n",
    "# Add the embeddings as layers after their respective inputs\n",
    "airline_vec = airline_emb(airline_encoded)            # shape --> (batch, 1, 3)\n",
    "airport_from_vec = airport_emb(airport_from_encoded)  # shape --> (batch, 1, 5)\n",
    "airport_to_vec = airport_emb(airport_to_encoded)      # shape --> (batch, 1, 5)\n",
    "\n",
    "# Flatten the embeddings, so that they can be concatenated with the numeric features\n",
    "airline_vec = tf.keras.layers.Flatten()(airline_vec)            # shape --> (batch, 3)\n",
    "airport_from_vec = tf.keras.layers.Flatten()(airport_from_vec)  # shape --> (batch, 5)\n",
    "airport_to_vec = tf.keras.layers.Flatten()(airport_to_vec)      # shape --> (batch, 5)\n",
    "\n",
    "# Concatenate the embeddings together with the numeric inputs\n",
    "concat = tf.keras.layers.Concatenate()([numeric_inp, airline_vec, airport_from_vec,\n",
    "                                        airport_to_vec])  # shape --> (batch, 17)\n",
    "\n",
    "# Add dense layers \n",
    "hid1 = tf.keras.layers.Dense(300, activation='relu')(concat)\n",
    "hid2 = tf.keras.layers.Dense(100, activation='relu')(hid1)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(hid2)\n",
    "\n",
    "# Define model. We need to define all the inputs we used.\n",
    "# We'll do this as a dict to be more safe when passing the values during training\n",
    "model = tf.keras.models.Model(inputs={'numeric': numeric_inp,\n",
    "                                      'airline': airline_inp,\n",
    "                                      'airport_from': airport_from_inp,\n",
    "                                      'airport_to': airport_to_inp},\n",
    "                              outputs=out)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision',\n",
    "                                                                     'Recall', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f81f5",
   "metadata": {},
   "source": [
    "A sketch of our model can be seen below.\n",
    "\n",
    "Let's prepare the dataset in the form that our model expects it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ac517ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "def convert_to_dict(df):\n",
    "    return {'numeric': df.drop(columns=['Airline', 'AirportFrom', 'AirportTo']).astype(float).values,\n",
    "            'airline': df['Airline'],\n",
    "            'airport_from': df['AirportFrom'],\n",
    "            'airport_to': df['AirportTo']}\n",
    "\n",
    "\n",
    "X_train = convert_to_dict(X_train)\n",
    "X_test = convert_to_dict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea56a6",
   "metadata": {},
   "source": [
    "Train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "013e8d54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2950/2950 [==============================] - 31s 10ms/step - loss: 0.6278 - accuracy: 0.6449 - precision: 0.6442 - recall: 0.4511 - auc: 0.6890 - val_loss: 0.6272 - val_accuracy: 0.6440 - val_precision: 0.6747 - val_recall: 0.3920 - val_auc: 0.6925\n",
      "Epoch 2/10\n",
      "2950/2950 [==============================] - 29s 10ms/step - loss: 0.6269 - accuracy: 0.6453 - precision: 0.6455 - recall: 0.4496 - auc: 0.6902 - val_loss: 0.6270 - val_accuracy: 0.6439 - val_precision: 0.6766 - val_recall: 0.3886 - val_auc: 0.6934\n",
      "Epoch 3/10\n",
      "2950/2950 [==============================] - 30s 10ms/step - loss: 0.6265 - accuracy: 0.6462 - precision: 0.6468 - recall: 0.4509 - auc: 0.6908 - val_loss: 0.6266 - val_accuracy: 0.6455 - val_precision: 0.6296 - val_recall: 0.5014 - val_auc: 0.6933\n",
      "Epoch 4/10\n",
      "2950/2950 [==============================] - 30s 10ms/step - loss: 0.6261 - accuracy: 0.6465 - precision: 0.6477 - recall: 0.4505 - auc: 0.6913 - val_loss: 0.6292 - val_accuracy: 0.6439 - val_precision: 0.6086 - val_recall: 0.5682 - val_auc: 0.6942\n",
      "Epoch 5/10\n",
      "2950/2950 [==============================] - 30s 10ms/step - loss: 0.6257 - accuracy: 0.6464 - precision: 0.6481 - recall: 0.4490 - auc: 0.6919 - val_loss: 0.6249 - val_accuracy: 0.6471 - val_precision: 0.6574 - val_recall: 0.4387 - val_auc: 0.6945\n",
      "Epoch 6/10\n",
      "2950/2950 [==============================] - 30s 10ms/step - loss: 0.6252 - accuracy: 0.6471 - precision: 0.6496 - recall: 0.4489 - auc: 0.6925 - val_loss: 0.6255 - val_accuracy: 0.6464 - val_precision: 0.6550 - val_recall: 0.4403 - val_auc: 0.6931\n",
      "Epoch 7/10\n",
      "2950/2950 [==============================] - 32s 11ms/step - loss: 0.6251 - accuracy: 0.6466 - precision: 0.6493 - recall: 0.4474 - auc: 0.6929 - val_loss: 0.6309 - val_accuracy: 0.6404 - val_precision: 0.7192 - val_recall: 0.3198 - val_auc: 0.6945\n",
      "Epoch 8/10\n",
      "2950/2950 [==============================] - 29s 10ms/step - loss: 0.6247 - accuracy: 0.6478 - precision: 0.6510 - recall: 0.4490 - auc: 0.6933 - val_loss: 0.6258 - val_accuracy: 0.6450 - val_precision: 0.6811 - val_recall: 0.3861 - val_auc: 0.6942\n",
      "Epoch 9/10\n",
      "2950/2950 [==============================] - 29s 10ms/step - loss: 0.6244 - accuracy: 0.6479 - precision: 0.6506 - recall: 0.4504 - auc: 0.6939 - val_loss: 0.6255 - val_accuracy: 0.6467 - val_precision: 0.6577 - val_recall: 0.4360 - val_auc: 0.6937\n",
      "Epoch 10/10\n",
      "2950/2950 [==============================] - 29s 10ms/step - loss: 0.6245 - accuracy: 0.6483 - precision: 0.6520 - recall: 0.4492 - auc: 0.6937 - val_loss: 0.6262 - val_accuracy: 0.6451 - val_precision: 0.6845 - val_recall: 0.3814 - val_auc: 0.6931\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e758e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5057/5057 [==============================] - 17s 3ms/step - loss: 0.6267 - accuracy: 0.6443 - precision: 0.6326 - recall: 0.4863 - auc: 0.6918\n",
      "Loss      : 0.63\n",
      "Accuracy  : 0.64\n",
      "Precision : 0.63\n",
      "Recall    : 0.49\n",
      "AUC       : 0.69\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "\n",
    "for name, value in zip(metrics, results):\n",
    "    print(f'{name:<10}: {value:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8e243",
   "metadata": {},
   "source": [
    "By adding these categorical features we managed to improve the model's performance bit. By tuning parameters such as the embedding dim and the vocabulary size, we might get an even better performance out of our embeddings.\n",
    "\n",
    "## Part 1: Practical aspects regarding model training\n",
    "\n",
    "### History callback\n",
    "\n",
    "You might have noticed in the previous training that I assigned the output of `model.fit()` to a variable called `hist`. This is called the [History callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History). This stores all information relevant to the model's training (i.e. the stuff that is printed on screen). This callback gives us access to this information, which we can use for analysis, visualizations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
