{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKCSJOez9Tx4"
   },
   "source": [
    "# 2. Custom training\n",
    "\n",
    "In TF101 we saw how we can define and train models through the Keras API. This is very convenient, but in some cases can be limiting. In this notebook we'll see how we can (a) build custom Neural Networks without involving Keras at all and (b) train these models without keras' `model.fit()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 23:11:27.981074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-16 23:11:28.120871: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-16 23:11:28.120888: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-16 23:11:28.785922: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-16 23:11:28.785984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-16 23:11:28.785991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__[0] == '2', 'this tutorial is for tensorflow versions of 2 or higher'\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks in TensorFlow (w/o Keras) \n",
    "\n",
    "Mimicking what we did in the first TF101 notebook, we'll try to recreate the same MLP and train it on the moons dataset.\n",
    "\n",
    "First let's define the dataset. We'll change things a little bit though, to make the scenario more realistic. In large scale applications we cannot afford loading the whole training set in memory. Instead we have a generator producing a single batch at a time. This is the topic of the next notebook, though we will do our best to simulate this behavior in the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "\n",
    "def moons_generator(batch_size=32, noise=0.2):\n",
    "    '''\n",
    "    Generator for producing a batch of 'moons' data\n",
    "    '''\n",
    "    while True:\n",
    "        yield make_moons(batch_size, noise=noise)\n",
    "    \n",
    "    \n",
    "x_train = moons_generator()\n",
    "x_test = moons_generator()\n",
    "# The names x_train and x_test are not accurate as these \n",
    "# generators return the labels along with the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we defined in keras is the following:\n",
    "\n",
    "```python\n",
    "inp = tf.keras.layers.Input(shape=(2,)) \n",
    "hid = tf.keras.layers.Dense(30, activation='relu')(inp)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(hid)\n",
    "\n",
    "mlp = tf.keras.models.Model(inp, out)\n",
    "```\n",
    "\n",
    "To recreate this in TensorFlow, we need to know that the `Dense` layer essentially performs the following operation\n",
    "\n",
    "$$\n",
    "z = f (X \\cdot W + b)\n",
    "$$\n",
    "\n",
    "So, in turn the whole network does the following\n",
    "\n",
    "$$\n",
    "h = \\text{ReLU} (X \\cdot W_h + b_h) \\\\\n",
    "\\hat y = \\sigma (h \\cdot W_o + b_o)\n",
    "$$\n",
    "\n",
    "where $W_h, b_h, W_o, b_o$ are the weight and biases of the hidden and output layers respectively and $X$ is the model's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tf.Tensor(\n",
      "[[0.93362725]\n",
      " [0.7597806 ]\n",
      " [0.50926185]\n",
      " [0.96909326]\n",
      " [0.9815643 ]\n",
      " [0.951013  ]\n",
      " [0.97385806]\n",
      " [0.6870211 ]\n",
      " [0.9766229 ]\n",
      " [0.6125538 ]\n",
      " [0.9129326 ]\n",
      " [0.94532955]\n",
      " [0.951406  ]\n",
      " [0.92570126]\n",
      " [0.8948641 ]\n",
      " [0.68440527]\n",
      " [0.7504297 ]\n",
      " [0.977184  ]\n",
      " [0.59780157]\n",
      " [0.8162962 ]\n",
      " [0.6382722 ]\n",
      " [0.9724078 ]\n",
      " [0.964869  ]\n",
      " [0.93438154]\n",
      " [0.5921625 ]\n",
      " [0.9243256 ]\n",
      " [0.9424068 ]\n",
      " [0.82952744]\n",
      " [0.9826905 ]\n",
      " [0.5754374 ]\n",
      " [0.7600825 ]\n",
      " [0.9289852 ]], shape=(32, 1), dtype=float32)\n",
      "Labels:\n",
      "[1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 23:11:30.167677: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-16 23:11:30.167697: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-16 23:11:30.167714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (home): /proc/driver/nvidia/version does not exist\n",
      "2023-04-16 23:11:30.167944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(13)\n",
    "\n",
    "# Hidden layer parameters\n",
    "W_h = tf.Variable(tf.random.normal(shape=(2, 30)))  # shape=(input_features, hidden_layer_neurons)\n",
    "b_h = tf.Variable(tf.random.normal(shape=(1, 30)))\n",
    "\n",
    "# Output layer parameters\n",
    "W_o = tf.Variable(tf.random.normal(shape=(30, 1)))  # shape=(hidden_layer_neurons, output)\n",
    "b_o = tf.Variable(tf.random.normal(shape=(1, 1)))\n",
    "\n",
    "# Create a list of all of the model's trainable parameters\n",
    "trainable_params = [W_h, b_h, W_o, b_o]\n",
    "\n",
    "\n",
    "# Hidden layer operations\n",
    "def hidden(x):\n",
    "    return tf.math.sigmoid(x @ W_h + b_h)  # @ --> matmul operator\n",
    "\n",
    "\n",
    "# Output layer operations\n",
    "def output(x):\n",
    "    return tf.math.sigmoid(x @ W_o + b_o)\n",
    "\n",
    "\n",
    "# Get a batch from the generator\n",
    "x, y = next(x_train)\n",
    "\n",
    "\n",
    "# Run predictions for random weights\n",
    "hidden_layer_outputs = hidden(x)\n",
    "y_hat = output(hidden_layer_outputs)\n",
    "\n",
    "print('Predictions:')\n",
    "print(y_hat)\n",
    "print('Labels:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the accuracy of this untrained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Define accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    bin_preds = (y_pred > 0.5).numpy().astype(int).flatten()\n",
    "    return np.sum(y_true == bin_preds) / len(y_true) \n",
    "\n",
    "# Compute the accuracy of the random classifier\n",
    "acc = accuracy(y, y_hat)\n",
    "print('Initial accuracy: {:.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "\n",
    "To make our life easier we'll define two functions to assist us during training and prediction. These two will perform the **forward** and **backward passes** for our network. The forward pass is the procedure that generates the prediction out of an input (i.e. the whole series of transformations from the beginning to the end of the network). The backward pass is the computation of the gradients of the loss w.r.t each one of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_true, y_pred):\n",
    "    '''\n",
    "    Compute the binary cross-entropy between y_true and y_pred\n",
    "    '''\n",
    "    return -tf.math.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    '''\n",
    "    Gererate the prediction for an input tensor x\n",
    "    '''\n",
    "    return output(hidden(x))\n",
    "\n",
    "\n",
    "def backward(x, y):\n",
    "    '''\n",
    "    Compute the gradients for the loss w.r.t the model's trainable parameters,\n",
    "    for an input tensor x\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = forward(x)\n",
    "        loss = bce(y, y_hat)\n",
    "    return tape.gradient(loss, trainable_params), loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `backward()` to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(2, 30), dtype=float32, numpy=\n",
       "  array([[-6.6126473e-02,  5.0442852e-02, -3.8183581e-02, -6.3892296e-03,\n",
       "           6.5928483e-03, -6.5499179e-02, -5.8066577e-02,  7.2945543e-02,\n",
       "          -5.2423910e-03, -7.0356302e-02,  4.7018182e-02, -6.3370340e-02,\n",
       "           2.7766356e-02, -8.9933688e-04,  1.1129200e-01, -1.3603233e-02,\n",
       "           8.8072853e-04,  2.1455297e-03,  1.4358440e-02, -3.9317971e-04,\n",
       "          -1.6874094e-02,  8.0551594e-02, -4.9527781e-03, -1.0886554e-02,\n",
       "          -5.7324495e-02,  1.4620623e-01,  1.2801398e-02, -2.2883579e-02,\n",
       "          -2.7264878e-03,  1.4228658e-03],\n",
       "         [-5.0796550e-03,  1.9862784e-02, -1.7627474e-02, -4.2240610e-03,\n",
       "           1.1561638e-03, -1.1534214e-02, -1.1625479e-02,  1.1771051e-02,\n",
       "          -1.2442167e-03, -5.3826235e-03,  1.4663624e-02, -1.2313918e-03,\n",
       "           6.1133262e-03, -4.3967398e-04,  2.0463310e-02, -2.5429826e-03,\n",
       "           5.6145922e-04,  6.3758274e-04,  2.6986217e-03, -7.4419848e-05,\n",
       "          -5.7204477e-03,  7.7386936e-03, -2.1440526e-03, -3.5172943e-03,\n",
       "          -1.0464095e-02,  2.7944079e-02,  1.8502386e-02, -1.3062207e-02,\n",
       "          -4.5162989e-04,  7.1477611e-04]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 30), dtype=float32, numpy=\n",
       "  array([[-0.06597981,  0.07494881, -0.06480771, -0.01362297,  0.01328863,\n",
       "          -0.07568298, -0.07104543,  0.08547492, -0.00698605, -0.07436064,\n",
       "           0.0862388 , -0.05801184,  0.03335883, -0.00264366,  0.13358016,\n",
       "          -0.01392157,  0.0019362 ,  0.00257828,  0.01692797, -0.00046404,\n",
       "          -0.03090856,  0.08068788, -0.01073005, -0.01654148, -0.06673723,\n",
       "           0.17008667,  0.04204392, -0.04529827, -0.00365338,  0.00312688]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "  array([[0.11146979],\n",
       "         [0.22540718],\n",
       "         [0.08391993],\n",
       "         [0.26266086],\n",
       "         [0.30758634],\n",
       "         [0.17708369],\n",
       "         [0.15489025],\n",
       "         [0.17128007],\n",
       "         [0.14238176],\n",
       "         [0.14126   ],\n",
       "         [0.07672643],\n",
       "         [0.0924576 ],\n",
       "         [0.19061649],\n",
       "         [0.28798947],\n",
       "         [0.17933327],\n",
       "         [0.09037542],\n",
       "         [0.06291863],\n",
       "         [0.13789609],\n",
       "         [0.16329867],\n",
       "         [0.14245547],\n",
       "         [0.05740597],\n",
       "         [0.20876722],\n",
       "         [0.07743511],\n",
       "         [0.2350351 ],\n",
       "         [0.17445938],\n",
       "         [0.15892853],\n",
       "         [0.28610376],\n",
       "         [0.09076645],\n",
       "         [0.2656054 ],\n",
       "         [0.29136914]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.3392592]], dtype=float32)>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2646654>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = backward(x, y)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we get a derivative for each parameter of the network. To train our, we'll use [**gradient descent**](https://en.wikipedia.org/wiki/Gradient_descent). The idea is to compute the partial derivative of the loss w.r.t the model's parameters (what we have done with the `GradientTape`) and update the parameters according to their partial derivative.\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\lambda \\frac{\\partial L(y, \\hat y)}{\\partial w}\n",
    "$$\n",
    "\n",
    "where $w$ is a weight in the netowrk, $L$ is our loss function (BCE in our case) and $\\lambda$ is the learning rate.\n",
    "\n",
    "Note that the partial derivative of $L$ w.r.t $w$ shows the *direction* that $w$ needs to be changed to increase the loss the most. The opposite (i.e. $- \\partial L / \\partial w$), shows how to decrease the loss the most. That's why we want to update our parameters in that direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_update(trainable_parameters, gradients, lr=0.1):\n",
    "    '''\n",
    "    Apply a single step of Gradient Descent to the model's trainable parameters.\n",
    "    \n",
    "    i.e. \n",
    "    \n",
    "        param = param - lr * grad \n",
    "    \n",
    "    for every param, grad pair\n",
    "    '''\n",
    "    \n",
    "    for p, g in zip(trainable_params, gradients):\n",
    "        p.assign_sub(lr * g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have all the components we need to build a custom training loop.\n",
    "\n",
    "## Custom training loops\n",
    "\n",
    "In TF101 we saw how we can easily train keras models through the `.fit()` API. This API essentially manages to **hide the training loop** from us, which is done intentionally to simplify the process of training Neural Networks.\n",
    "\n",
    "What would normally be\n",
    "\n",
    "```python\n",
    "for e in range(epochs):\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        # ...\n",
    "        # train on batch\n",
    "        # ...\n",
    "```\n",
    "\n",
    "in Keras becomes\n",
    "\n",
    "```python\n",
    "model.fit(x_train, y_train, epochs=epochs)\n",
    "```\n",
    "\n",
    "While this achieves a great deal of simplicity, it comes in the expense of **control**. To affect some aspects of the training process (e.g. learning rate) or to monitor variables, we need to use one of the existing or write a custom callback.\n",
    "\n",
    "Still there are limitations to what we can affect or monitor through the use of callbacks! For this reason, in some cases we might want to train the model in a custom loop, where we have full control.\n",
    "\n",
    "The fundamental steps we need to perform **at each iteration** are:\n",
    "\n",
    "1. Generate the training **batch** (samples + labels).\n",
    "2. Perform the **forward pass** and get the model's **predictions** for this batch.\n",
    "3. Calculate the **loss** of these predictions compared to the actual labels.\n",
    "4. Compute the loss' **gradients** w.r.t the model's trainable parameters (i.e. **backprop**).\n",
    "5. **Update** the model's parameters according to the gradients (i.e. use the **optimizer**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps_per_epoch = 10  # this normally is the amount of steps \n",
    "                      # it takes to exhaust the training set\n",
    "val_steps_per_epoch = 5   # same but for the validataion set\n",
    "learning_rate = 0.1\n",
    "metrics = {'loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Here we normally would shuffle the training set\n",
    "    \n",
    "    # Training \n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        x, y = next(x_train)  # get next batch from generator\n",
    "        \n",
    "        grads, loss = backward(x, y)  # forward and backward passes\n",
    "        \n",
    "        gd_update(trainable_params, grads, learning_rate)  # parameter updates\n",
    "        \n",
    "        metrics['loss'].append(loss.numpy())\n",
    "        \n",
    "        \n",
    "    # Validataion\n",
    "    for step in range(val_steps_per_epoch):\n",
    "        tmp_loss, tmp_acc = [], []\n",
    "        \n",
    "        x, y = next(x_test)  # get batch from validataion data generator\n",
    "        \n",
    "        y_hat = forward(x)  # get the model's predicions\n",
    "        \n",
    "        # Compute the loss and accuracy for this batch\n",
    "        tmp_loss.append(bce(y, y_hat).numpy())\n",
    "        tmp_acc.append(accuracy(y, y_hat))\n",
    "        \n",
    "    # Aggregate over all batches    \n",
    "    metrics['val_loss'].append(np.mean(tmp_loss))\n",
    "    metrics['val_accuracy'].append(np.mean(tmp_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss progression thourgh training')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0, epochs * steps_per_epoch), metrics['loss'], label='train')\n",
    "plt.plot(range(0, epochs * steps_per_epoch, steps_per_epoch), metrics['val_loss'], label='validation')\n",
    "\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss progression thourgh training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvisouly, we don't need to do everything from scratch (defining the loss function, doing the weight udpates, etc.). We can combine the flexibility of the custom training loop with the ease of defining models the Keras way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(2,)) \n",
    "hid = tf.keras.layers.Dense(30, activation='relu')(inp)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(hid)\n",
    "\n",
    "mlp = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the training logic. We'll first define a function that does a single training step for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of defining the loss function, metrics, etc. ourselves we'll use the Keras ones\n",
    "\n",
    "loss_function = tf.keras.losses.binary_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "# The metrics in Keras have some big differences to the one we defined on our own, \n",
    "# but we'll ignore them for now...\n",
    "\n",
    "\n",
    "# We'll now define a function that performs a single training step. We don't\n",
    "# need to apply the tf.function decorator here, because this function isn't \n",
    "# intended to be a top-level function (i.e. it will be called from another).\n",
    "def train_on_batch(x, y):\n",
    "    '''\n",
    "    Will train 'mlp' as defined in the global scope for a single training batch.\n",
    "    The optimizer and loss function will also be taken from the global scope.\n",
    "    '''\n",
    "    \n",
    "    # Start monitoring the operations in order to compute the gradient:\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Generate the model's prediction:\n",
    "        y_hat = mlp(x)\n",
    "\n",
    "        # Calculate its loss:\n",
    "        loss = loss_function(y, y_hat)\n",
    "\n",
    "    # Compute the gradient of the loss w.r.t the model's parameters\n",
    "    grads = tape.gradient(loss, mlp.trainable_variables)\n",
    "\n",
    "    # Update the model's parameters through the optimizer\n",
    "    optimizer.apply_gradients(zip(grads, mlp.trainable_variables))\n",
    "\n",
    "    # Store the batch's accuracy\n",
    "    acc = accuracy(tf.squeeze(y), tf.squeeze(y_hat))\n",
    "\n",
    "    # Get the batch's loss so that we can plot it later\n",
    "    return tf.reduce_mean(loss), tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a function that uses the previous to train the model for a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(train_data, epochs, validation_data=None):\n",
    "    '''\n",
    "    Train 'mlp' as defined in the global scope for a number of epochs, on a \n",
    "    given dataset. \n",
    "    '''\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        tf.print('Epoch {} of {}'.format(e+1, epochs))\n",
    "    \n",
    "        loss = 0.\n",
    "        acc = 0.\n",
    "\n",
    "        # Training epoch:\n",
    "        for i, (x, y) in enumerate(train_data):\n",
    "\n",
    "            # Perform a single iteration\n",
    "            loss, acc = train_on_batch(x, y.reshape(-1, 1))\n",
    "\n",
    "            # We need to manually keep track of the iteration,\n",
    "            # because this generator will loop forever \n",
    "            if i >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                tf.print('    Iteration:', i)\n",
    "                tf.print('        Training loss:', loss)\n",
    "                tf.print('        Training acc:', acc)\n",
    "\n",
    "        if validation_data:\n",
    "\n",
    "            valid_acc = 0.\n",
    "\n",
    "            # Iterate over the validation set:\n",
    "            for i, (x, y) in enumerate(validation_data):\n",
    "\n",
    "                # Make a prediction and compute the mean of the accuracy and the loss\n",
    "                preds = mlp(x)\n",
    "\n",
    "                # Compute the mean accuracy of each batch and add them to 'valid_acc'\n",
    "                valid_acc += accuracy(y, preds)\n",
    "\n",
    "                # Again keep track of the iteration in order to terminate the loop\n",
    "                if i >= val_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            tf.print('    Validation accuracy:', valid_acc / tf.cast(i, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.782895923\n",
      "        Training acc: 0.15625\n",
      "    Validation accuracy: 0.31296739\n",
      "Epoch 2 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.7483868\n",
      "        Training acc: 0.263888896\n",
      "    Validation accuracy: 0.367396921\n",
      "Epoch 3 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.716344833\n",
      "        Training acc: 0.316964298\n",
      "    Validation accuracy: 0.434734792\n",
      "Epoch 4 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.693190455\n",
      "        Training acc: 0.373197109\n",
      "    Validation accuracy: 0.486119419\n",
      "Epoch 5 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.670165539\n",
      "        Training acc: 0.413949281\n",
      "    Validation accuracy: 0.541246414\n",
      "Epoch 6 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.649497628\n",
      "        Training acc: 0.459665686\n",
      "    Validation accuracy: 0.591187537\n",
      "Epoch 7 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.618050575\n",
      "        Training acc: 0.499696612\n",
      "    Validation accuracy: 0.631009281\n",
      "Epoch 8 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.599900424\n",
      "        Training acc: 0.532031238\n",
      "    Validation accuracy: 0.66367805\n",
      "Epoch 9 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.589272082\n",
      "        Training acc: 0.557938\n",
      "    Validation accuracy: 0.688966632\n",
      "Epoch 10 of 10\n",
      "    Iteration: 0\n",
      "        Training loss: 0.538722575\n",
      "        Training acc: 0.578733742\n",
      "    Validation accuracy: 0.711734951\n"
     ]
    }
   ],
   "source": [
    "train(x_train, epochs=10, validation_data=x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard and custom training loops\n",
    "\n",
    "Training with a custom training loop gives us a greater flexibility in choosing what to log to TensorBoard and when. To log stuff to tensorboard, we use the `tf.summary` API.\n",
    "\n",
    "There are two steps we need to do to use this API:\n",
    "1. create a summary file writer\n",
    "2. add the thing you want to log to this writer\n",
    "\n",
    "`tf.summary` has different functions for the different things we want to write to TensorBoard, such as:\n",
    "- `tf.summary.scalar`: log scalar values (e.g. metrics, losses).\n",
    "- `tf.summary.histogram`: log histograms (e.g. histogram of weights, activations).\n",
    "- `tf.summary.image`: log images. Can be used to log the output of generative models or even input images to see what the augmentations we are using actually look like.\n",
    "- `tf.summary.text`: log text. Can be used in similar situations as the above.\n",
    "- `tf.summary.audio`: log audio. Can be used in similar situations as the above.\n",
    "- `tf.summary.graph`: log the TF computational graph. \n",
    "\n",
    "Let's see how these work. First we need to create the summary writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer('logs/custom_loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging scalars\n",
    "\n",
    "The most common thing we want to log is scalar values. These can be losses, metrics or anything else we might want to log. \n",
    "\n",
    "In the example below we'll log 3 things:\n",
    "- The value of the loss at each iteration\n",
    "- The accuracy of our model at each iteration $^1$\n",
    "- The value of the learning rate at each iteration. We will define an exponential decaying learning rate, so that we have something other than a constant line to plot.\n",
    "\n",
    "$^1$ *Technically the Keras accuracy metric doesn't do exactly that; we'll explore this in a future notebook. For now let's assume that this is what it returns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the components\n",
    "loss_function = tf.keras.losses.binary_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "\n",
    "\n",
    "# Define an exponentially decaying LR\n",
    "def build_exponentially_decayed_lr_func(initial_lr, decay_rate, decay_steps):\n",
    "    '''\n",
    "    Builds a function that given a training step, returns the value of the LR\n",
    "    '''\n",
    "    def lr_func(step):\n",
    "        return initial_lr * decay_rate ** (step / decay_steps)\n",
    "\n",
    "    return lr_func\n",
    "\n",
    "\n",
    "lr_schedule = build_exponentially_decayed_lr_func(initial_lr=0.1, \n",
    "                                                  decay_rate=0.01,\n",
    "                                                  decay_steps=100)\n",
    "\n",
    "\n",
    "def log_scalars(loss, accuracy, lr, step):\n",
    "    '''\n",
    "    Log our metrics as scalars to the summary writer\n",
    "    '''\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', loss, step=step)\n",
    "        tf.summary.scalar('accuracy', accuracy, step=step)\n",
    "        tf.summary.scalar('learning_rate', lr, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would play out something like this in our training loop:\n",
    "\n",
    "```python\n",
    "# Dummy training loop\n",
    "for i, (x, y) in enumerate(x_train):\n",
    "\n",
    "\n",
    "    lr = lr_schedule(i)                 # get new value of LR\n",
    "    optimizer.learning_rate.assign(lr)  # change optimizer's LR\n",
    "\n",
    "    loss, acc = train_on_batch(x, y.reshape(-1, 1))\n",
    "\n",
    "    log_scalars(loss, acc, lr, i)  # log the scalars\n",
    "\n",
    "    # Finish training after 100 steps\n",
    "    if i >= 100:\n",
    "        break\n",
    "```\n",
    "\n",
    "\n",
    "### Logging images\n",
    "\n",
    "Another interesting thing we can do is to log actual images in TensorBoard. This is more useful in generative models that produce images. Nevertheless there can be other situations where this might be useful. In this example we'll plot a confusion matrix for our model and log it.\n",
    "\n",
    "The example was adapted from [this blog](https://towardsdatascience.com/a-quickstart-guide-to-tensorboard-fb1ade69bbcf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "\n",
    "def plot_to_image(figure):\n",
    "    '''\n",
    "    Convert a matplotlib Figure to a png image\n",
    "    '''\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(figure)                    # Closing the figure prevents it from \n",
    "                                         # being displayed directly inside the notebook.\n",
    "    buf.seek(0)\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm):\n",
    "    '''\n",
    "    Plot a confusion matrix, given its values \n",
    "    '''\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm.T, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1])\n",
    "    plt.yticks([0, 1])\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[1]), range(cm.shape[0])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('True Label')\n",
    "    plt.ylabel('Predicted Label')\n",
    "    return figure\n",
    "\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, step):\n",
    "    '''\n",
    "    Compute the confusion matrix for a binary classification task and\n",
    "    log it to TensorBoard\n",
    "    '''\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, (y_pred > 0.5).numpy().astype(int))\n",
    "    \n",
    "    # Plot it\n",
    "    figure = plot_confusion_matrix(cm)\n",
    "    \n",
    "    # Convert it to an image\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log this image to TensorBoard\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.image('Confusion Matrix', cm_image, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would use the last function like this training loop:\n",
    "\n",
    "```python\n",
    "# Dummy training loop\n",
    "for i, (x, y) in enumerate(x_train):\n",
    "\n",
    "    loss, acc = train_on_batch(x, y.reshape(-1, 1))\n",
    "\n",
    "    # Log the confusion matrix every 20 steps\n",
    "    if i % 20 == 0:  \n",
    "        y_hat = mlp(x)  # it would be better to modify 'train_on_batch' to\n",
    "                        # return y_hat instead of needing to recompute it\n",
    "        log_confusion_matrix(y, y_hat, i)\n",
    "\n",
    "    # Finish training after 100 steps\n",
    "    if i >= 100:\n",
    "        break\n",
    "```\n",
    "\n",
    "### Logging histograms\n",
    "\n",
    "In some cases where we want to debug or research our model's performance, we might want to visualize how training affects the distribution of some weight matrix, activation or gradient vector in the model. The best way to do this is through histograms.\n",
    "\n",
    "We will do this for the weights, gradients of the first Dense layer in our MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dense_layer_histograms(step, grads):\n",
    "    '''\n",
    "    Log weights and gradient of the first hidden layer of our MLP\n",
    "    '''\n",
    "    weights = mlp.layers[1].get_weights()[0]\n",
    "    grad = grads[0]\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.histogram('Weights (1st hidden layer)', weights, step=step)\n",
    "        tf.summary.histogram('Gradient (1st hidden layer)', grad, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be used in a similar way to the previous two in our training loop:\n",
    "\n",
    "```python\n",
    "# Dummy training loop\n",
    "for i, (x, y) in enumerate(x_train):\n",
    "\n",
    "    loss, acc = train_on_batch(x, y.reshape(-1, 1))\n",
    "\n",
    "    # Log the histograms every 10 steps\n",
    "    if i % 10 == 0:  \n",
    "        log_dense_layer_histograms(i, grad)\n",
    "\n",
    "    # Finish training after 100 steps\n",
    "    if i >= 100:\n",
    "        break\n",
    "```\n",
    "\n",
    "### Example training loop with logging\n",
    "\n",
    "Putting these previous use-cases together we can build a training loop that will log images, scalars and histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy training loop\n",
    "for i, (x, y) in enumerate(x_train):\n",
    "\n",
    "\n",
    "    lr = lr_schedule(i)                 # get new value of LR\n",
    "    optimizer.learning_rate.assign(lr)  # change optimizer's LR\n",
    "\n",
    "    loss, acc = train_on_batch(x, y.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    # Log the scalars\n",
    "    log_scalars(loss, acc, lr, i)  \n",
    "    \n",
    "    # Log the confusion matrix every 20 steps\n",
    "    if i % 20 == 0:  \n",
    "        y_hat = mlp(x)  # it would be better to modify 'train_on_batch' to\n",
    "                        # return y_hat instead of needing to recompute it\n",
    "        log_confusion_matrix(y, y_hat, i)\n",
    "\n",
    "    # Log the histograms every 10 steps\n",
    "    if i % 10 == 0:  \n",
    "        log_dense_layer_histograms(i, grads)\n",
    "\n",
    "    # Finish training after 100 steps\n",
    "    if i >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dd53ef700f0e2bd1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dd53ef700f0e2bd1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP87Qinm/KeQGB6t1IVYdvn",
   "collapsed_sections": [],
   "name": "01_tensors_and_low_level_ops.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
